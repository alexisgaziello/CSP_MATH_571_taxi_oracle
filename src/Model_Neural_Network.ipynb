{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to get reproducible results\n",
    "\n",
    "# Seed value (can actually be different for each attribution step)\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    def dummie_and_drop(df, name):\n",
    "        # Creates a dummy variable, concatenates it and finally drops the original categorical variable.\n",
    "        # In order not to have redundant variables, one of the dummy variables is dropped too\n",
    "        dummies = pd.get_dummies(df[name]).rename(columns = lambda x: name + '_' + str(x))\n",
    "        dummies = dummies.drop(dummies.columns[-1], axis = 1)\n",
    "        df = pd.concat([df, dummies], axis = 1)\n",
    "        df.drop(columns = [name], inplace=True, axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def convert_to_categorical(df, categorical_variables, categories, need_pickup = True):\n",
    "        \"\"\" \n",
    "        The dataframe's selected variables are converted to categorical, and each variable's categories are also specified.\n",
    "        It is also specified if the \"pickup community area\" has to be converted into categorical or no. If it is not \n",
    "        converted into categorical it is because it's not going to be used in the model.            \n",
    "        \"\"\"\n",
    "        \n",
    "        if need_pickup:\n",
    "            begin = 0\n",
    "        else:\n",
    "            df.drop(columns = ['pickup_community_area'], inplace = True, axis = 1)\n",
    "            begin = 1\n",
    "        \n",
    "        for i in range(begin, len(categorical_variables)):\n",
    "            df[categorical_variables[i]] = df[categorical_variables[i]].astype('category').cat.set_categories(categories[i])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def load(name, need_pickup = False, drop_correlated = False):\n",
    "    \n",
    "        # This parameter has to be set to True if the \"pickup_community_area\" variable is needed in the model\n",
    "        \n",
    "\n",
    "        # Load needed dataset and choose the useful columns\n",
    "        df = pd.read_csv(name) #'dataset_train.csv')\n",
    "        x = df[['pickup_community_area' ,'temperature', 'relative_humidity', 'wind_direction', 'wind_speed', 'precipitation_cat', \n",
    "                'sky_level', 'daytype', 'Day Name', 'Month', 'Quarter', 'Hour', 'Fare Last Month', 'Trips Last Hour',\n",
    "                'Trips Last Week (Same Hour)', 'Trips 2 Weeks Ago (Same Hour)', 'Year', 'trip_start_timestamp']]\n",
    "\n",
    "        # Convert the categorical variables\n",
    "        categorical_variables = ['pickup_community_area', 'daytype', 'sky_level', 'Day Name', 'Month','Hour', 'Year']\n",
    "        categories = [[*(range(1,78))], ['U', 'W', 'A'], ['OVC', 'BKN', 'SCT', 'FEW', 'CLR', 'VV '], \n",
    "                      ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], \n",
    "                      [*(range(1,13))], [*(range(0, 24))], [2017, 2018, 2019]]\n",
    "\n",
    "        x = convert_to_categorical(x, categorical_variables, categories, need_pickup = need_pickup)\n",
    "\n",
    "        \n",
    "        # Make dummy variables with the categorical ones\n",
    "        if need_pickup:\n",
    "            begin = 0\n",
    "        else:\n",
    "            begin = 1\n",
    "        for i in range(begin, len(categorical_variables)):\n",
    "            x = dummie_and_drop(x, name = categorical_variables[i])\n",
    "\n",
    "        y = df['Trips'].to_numpy()\n",
    "\n",
    "        if need_pickup == False:\n",
    "            # If we don't need the pickup, it means this is Neural Network case. Therefore we have to modify Y, in order\n",
    "            # to have \"n_areas\" outputs per input (because there are \"n_areas\" regressions per input)\n",
    "            x = x.groupby(by = 'trip_start_timestamp').mean()\n",
    "            n_areas = 77\n",
    "            y = np.reshape(y, [-1, n_areas]) # If \n",
    "        else:\n",
    "            x.drop(columns = ['trip_start_timestamp'], inplace = True, axis = 1)\n",
    "        \n",
    "        if drop_correlated:\n",
    "            x.drop(columns = ['Quarter'], inplace = True, axis = 1)\n",
    "            x.drop(columns = ['Trips Last Week (Same Hour)'], inplace = True, axis = 1)\n",
    "            x.drop(columns = ['Trips 2 Weeks Ago (Same Hour)'], inplace = True, axis = 1)\n",
    "        else:\n",
    "            x['Quarter'] = df['Quarter'].astype('category').cat.set_categories([*range(0,4)])\n",
    "\n",
    "        x = x.to_numpy()\n",
    "        \n",
    "        return (x,y)   \n",
    "    \n",
    "\n",
    "    need_pickup = False \n",
    "    drop_correlated = False\n",
    "    \n",
    "    \n",
    "    name_train = 'dataset_train.csv'\n",
    "    name_test = 'dataset_test.csv'\n",
    "    x, y = load(name_train, need_pickup, drop_correlated)\n",
    "    x_test, y_test = load(name_test, need_pickup, drop_correlated)\n",
    "    \n",
    "    \n",
    "    return (x, x_test, y, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs_julen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "x, x_test, y, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_taxi():\n",
    "    \n",
    "    def dummie_and_drop(df, name):\n",
    "        # Creates a dummy variable, concatenates it and finally drops the original categorical variable.\n",
    "        # In order not to have redundant variables, one of the dummy variables is dropped too\n",
    "        dummies = pd.get_dummies(df[name]).rename(columns = lambda x: name + '_' + str(x))\n",
    "        dummies = dummies.drop(dummies.columns[-1], axis = 1)\n",
    "        df = pd.concat([df, dummies], axis = 1)\n",
    "        df.drop(columns = [name], inplace=True, axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def convert_to_categorical(df, categorical_variables, categories, need_pickup = True):\n",
    "        \"\"\" \n",
    "        The dataframe's selected variables are converted to categorical, and each variable's categories are also specified.\n",
    "        It is also specified if the \"pickup community area\" has to be converted into categorical or no. If it is not \n",
    "        converted into categorical it is because it's not going to be used in the model.            \n",
    "        \"\"\"\n",
    "        \n",
    "        if need_pickup:\n",
    "            begin = 0\n",
    "        else:\n",
    "            df.drop(columns = ['pickup_community_area'], inplace = True, axis = 1)\n",
    "            begin = 1\n",
    "        \n",
    "        for i in range(begin, len(categorical_variables)):\n",
    "            df[categorical_variables[i]] = df[categorical_variables[i]].astype('category').cat.set_categories(categories[i])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def load(name, need_pickup = False, drop_correlated = False):\n",
    "    \n",
    "        # This parameter has to be set to True if the \"pickup_community_area\" variable is needed in the model\n",
    "        need_pickup = False \n",
    "\n",
    "        # Load needed dataset and choose the useful columns\n",
    "        df = pd.read_csv(name) #'dataset_train.csv')\n",
    "        x = df[['pickup_community_area', 'Day Name', 'Month', 'Hour', 'Fare Last Month', 'Tips Last Month', \n",
    "                'Trips Last Hour', 'Trips Last Week (Same Hour)', 'Trips 2 Weeks Ago (Same Hour)', 'Year']]\n",
    "\n",
    "        # Convert the categorical variables\n",
    "        categorical_variables = ['pickup_community_area', 'Day Name', 'Month','Hour', 'Year']\n",
    "        categories = [[*(range(1,78))], ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], \n",
    "                      [*(range(1,13))], [*(range(0, 24))], ['2017', '2018', '2019']]\n",
    "\n",
    "        x = convert_to_categorical(x, categorical_variables, categories, need_pickup = need_pickup)\n",
    "\n",
    "        \n",
    "        # Make dummy variables with the categorical ones\n",
    "        if need_pickup:\n",
    "            begin = 0\n",
    "        else:\n",
    "            begin = 1\n",
    "        for i in range(begin, len(categorical_variables)):\n",
    "            x = dummie_and_drop(x, name = categorical_variables[i])\n",
    "\n",
    "        y = df['Trips'].to_numpy()\n",
    "\n",
    "        if need_pickup == False:\n",
    "            # If we don't need the pickup, it means this is Neural Network case. Therefore we have to modify Y, in order\n",
    "            # to have \"n_areas\" outputs per input (because there are \"n_areas\" regressions per input)\n",
    "            n_areas = 77\n",
    "            y = np.reshape(y, [-1, n_areas]) \n",
    "            \n",
    "        if drop_correlated:\n",
    "            x.drop(columns = ['Trips Last Week (Same Hour)'], inplace = True, axis = 1)\n",
    "            x.drop(columns = ['Trips 2 Weeks Ago (Same Hour)'], inplace = True, axis = 1)\n",
    "\n",
    "        x = x.to_numpy()\n",
    "        \n",
    "        return (x,y)\n",
    "\n",
    "    need_pickup = True \n",
    "    drop_correlated = True\n",
    "    \n",
    "    name_train = 'dataset_train.csv'\n",
    "    name_test = 'dataset_test.csv'\n",
    "    x_train, y_train = load(name_train, need_pickup, drop_correlated)\n",
    "    x_test, y_test = load(name_test, need_pickup, drop_correlated)\n",
    "    \n",
    "    \n",
    "    return (x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['mae'])\n",
    "    plt.plot(history.history['val_mae'])\n",
    "    plt.title('Model mean absolute error')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x, x_test, y, y_test): #n_areas, features, x_train, y_train):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    \n",
    "    # In order to get reproducible results\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(2020)\n",
    "    from numpy.random import seed\n",
    "    seed(1)\n",
    "    \n",
    "    n_areas = y.shape[1]\n",
    "    features = x.shape[1]\n",
    "    \n",
    "    act = {{choice(['relu', 'sigmoid', 'tanh'])}} # Choose the activation function\n",
    "    \n",
    "    model = Sequential()\n",
    "    # Choose the architecture\n",
    "    architecture = {{choice(['arc_0', 'arc_1', 'arc_2', 'arc_3', 'arc_4'])}}\n",
    "    \n",
    "    if architecture == 'arc_0':\n",
    "        model.add(Dense(32, activation = act, input_shape = (features,)))\n",
    "        model.add(Dense(64, activation= act))\n",
    "        model.add(Dense(32, activation= act))\n",
    "    \n",
    "    elif architecture == 'arc_1':\n",
    "        model.add(Dense(64, activation = act, input_shape = (features,)))\n",
    "        model.add(Dense(128, activation= act))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        model.add(Dense(128, activation= act))\n",
    "    \n",
    "    elif architecture == 'arc_2':\n",
    "        model.add(Dense(128, activation = act, input_shape = (features,)))\n",
    "        model.add(Dense(128, activation= act))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        \n",
    "    elif architecture == 'arc_3':\n",
    "        model.add(Dense(128, activation = act, input_shape = (features,)))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(1024, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        model.add(Dense(128, activation= act))\n",
    "        \n",
    "    elif architecture == 'arc_4':\n",
    "        model.add(Dense(256, activation = act, input_shape = (features,)))\n",
    "        model.add(Dense(256, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(1024, activation= act))\n",
    "        model.add(Dense(1024, activation= act))\n",
    "        model.add(Dense(512, activation= act))\n",
    "        model.add(Dense(256, activation= act))\n",
    "\n",
    "    model.add(Dense(n_areas))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer = {{choice(['adam', 'rmsprop' , 'sgd'])}}, loss = 'mse', metrics = ['mae'])\n",
    "    model.summary()\n",
    "    \n",
    "    # checkpoint\n",
    "#     filepath=\"weights-improvement-{epoch:02d}-{val_mae:.2f}.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n",
    "#     callbacks_list = [checkpoint]\n",
    "    \n",
    "#     model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose = 0) #callbacks=callbacks_list, verbose=0)\n",
    "    \n",
    "    result = model.fit(x = x, y = y, validation_split = 0.15, \n",
    "                        batch_size = {{choice([32, 64, 128])}},\n",
    "                        epochs = 200, verbose = 0)\n",
    "    \n",
    "    validation_mae = np.amin(result.history['val_mae']) \n",
    "    print('Best validation mae of epoch:', validation_mae)\n",
    "    return {'loss': validation_mae, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exec('from __future__ import absolute_import, division, print_function')\n",
    "import numpy as np\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "best_run, best_model = optim.minimize(model = create_model,\n",
    "                                      data = load_data,\n",
    "                                      algo = tpe.suggest, \n",
    "                                      max_evals = 8,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name = 'Model_Neural_Network')\n",
    "x, x_test, y, y_test = load_data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n",
    "\n",
    "# plot_results(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(best_model.predict(x[0:1])).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.evaluate(x_test[0:1], y_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.predict(x_test[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# serialize model to JSON\n",
    "model_json = best_model.to_json()\n",
    "with open(\"model_neural_network.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "best_model.save_weights(\"model_neural_network.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
